# ğŸŒŒ Multimodal Reasoning with QwenVL

<p align="center">  
  <img src="https://img.shields.io/badge/Model-QwenVL-blueviolet?style=for-the-badge"/>  
  <img src="https://img.shields.io/badge/Task-VQA--SA-green?style=for-the-badge"/>  
  <img src="https://img.shields.io/badge/Competition-ICCV%20Multimodal%20Reasoning-orange?style=for-the-badge"/>  
</p>  

---

## ğŸ“– Introduction

This repository reproduces and extends experiments on **Visual Question Answering with Spatial Awareness (VQA-SA)**.
We evaluate **QwenVL2.5 (3B, 7B, 32B)** under different image resolutions to test modelsâ€™ abilities in:

- âœ¨ **Spatial reasoning** â€“ relative positions between entities  
- ğŸ§  **Commonsense reasoning** â€“ contextual understanding  
- ğŸ”® **Counterfactual reasoning** â€“ â€œwhat-ifâ€

Inspired by the **ICCV Multimodal Reasoning Competition**, this work explores how multimodal LLMs perceive physical space.

---

## ğŸ¯ Example

<p align="center">  
  <img src="images/12_0_101_0000001_240107.jpg" width="400"/>  
</p>  

Input JSON:

```json
[
  {
    "image_path": "images\\12_0_101_0000001_240107.jpg",
    "question": "ç©¿ç™½è‰²è¡£æœçš„äººåœ¨ç©¿æ£•è‰²è¡£æœçš„äººçš„å“ªä¸ªæ–¹å‘ï¼Ÿè¯·ä»å‰æ–¹ã€åæ–¹ã€å·¦æ–¹ã€å³æ–¹ã€ä¸Šæ–¹ã€ä¸‹æ–¹ã€å·¦å‰æ–¹ã€å³å‰æ–¹ã€å·¦åæ–¹ã€å³åæ–¹ä¸­é€‰æ‹©"
  },
  {
    "image_path": "images\\12_0_101_0000001_240107.jpg",
    "question": "ç©¿ç™½è‰²è¡£æœçš„äººåœ¨ç©¿æ£•è‰²è¡£æœçš„äººçš„ä¹‹é—´çš„è·ç¦»æœ‰10ç±³ä»¥ä¸Šå—ï¼Ÿè¯·å›ç­”æ˜¯æˆ–å¦"
  },
  {
    "image_path": "images\\12_0_101_0000001_240107.jpg",
    "question": "å›¾ä¸­è·¯ç¯åœ¨ç©¿æ£•è‰²è¡£æœçš„äººçš„å“ªä¸ªæ–¹å‘ï¼Ÿè¯·ä»å‰æ–¹ã€åæ–¹ã€å·¦æ–¹ã€å³æ–¹ã€ä¸Šæ–¹ã€ä¸‹æ–¹ã€å·¦å‰æ–¹ã€å³å‰æ–¹ã€å·¦åæ–¹ã€å³åæ–¹ä¸­é€‰æ‹©"
  }
]
```

ğŸ’¡ The model outputs **direction**, **yes/no judgment**, and **spatial relation** reasoning.

---

## ğŸš€ Features

* ğŸ”¹ **Supports QwenVL3 3B / 7B / 32B**
* ğŸ”¹ **Flexible image resolution experiments:[644*644,980*980,1288*1288,1624*1624]**
* ğŸ”¹ **Humanâ€“object spatial reasoning** (direction, distance)
* ğŸ”¹ **Extensible JSON format for questions**
* ğŸ”¹ **Reproducible benchmarking pipeline**

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ images/                # Dataset images
â”œâ”€â”€ configs/               # Model configs
â”œâ”€â”€ results/               # Experiment results & logs
â”œâ”€â”€ run_vqa.py             # Main entry for evaluation
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # Project description
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/your-username/multimodal-vqa-sa.git
cd multimodal-vqa-sa

conda create -n vqasa python=3.10
conda activate vqasa

pip install -r requirements.txt
```

---

## ğŸ–¥ï¸ Usage

ğŸ‘‰ **Question**

```bash
python run_vqa.py
```


## ğŸŒŸ Acknowledgements

* [QwenVL](https://github.com/QwenLM/Qwen2.5-VL) â€“ pretrained multimodal LLMs
* [ICCV Multimodal Reasoning Competition](https://lens4mllms.github.io/mars2-workshop-iccv2025/) â€“ benchmark inspiration

---

## ğŸ“œ License

This project is released under the **MIT License**.

